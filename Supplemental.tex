\subsection*{Estimation of a population of models using Pareto Optimal Ensemble Techniques (POETs).}

We used multiobjective optimization to estimate an ensemble of prostate models. Although computationally more complex than single-objective formulations, multiobjective optimization can be used to address qualitative conflicts in training data arising from experimental error or cell-line artifacts \cite{Handl2007}. In this study we used the Pareto Optimal Ensemble Technique (POETs) to perform the optimization. POETs integrates standard search strategies, e.g., Simulated Annealing (SA) or Local Pattern Search (PS) with a Pareto-rank fitness assignment \cite{Song2010}. The mean squared error, $\eta$, of parameter set $\mathbf{k}$ for training objective $j$ was defined as:    

\begin{equation}
\eta_j(\mathbf{p}_k) = \frac{1}{N}\sum_{i}^{N}\frac{(\hat{x}_{i,j} - \beta_j x(\mathbf{p_k})_{i,j})^2}{\hat{\sigma}^2_{i,j}}
\end{equation}
The symbol $\hat{x}_{i,j}$ denotes scaled experimental observations (from training objective j) while $x(\mathbf{p_k})_{i,j}$ denotes the simulation output (from training objective j). The quantity $i$ denotes the sampled time-index or condition, and $N$ denotes the number of time points or conditions for experiment j. The standard deviation, $\hat{\sigma}_{i,j}$, was assumed to be equal to 10\% of the reported observation, if no experimental error was reported. $\beta_j$ is a scaling factor which is required when considering experimental data that is accurate only to a multiplicative constant. In this study, the experimental data used for training and validation was typically band intensity from immunoblots, where intensity was estimated using the ImageJ software package \cite{Abramoff2004}. The scaling factor used was chosen to minimize the normalized squared error \cite{Brown2003}:

\begin{equation}
\beta_j = \frac{\sum_{i}(\hat{x}_{i,j}x_{i,j}/\hat{\sigma}_{i,j}^2)}{\sum_{i}(x_{i,j}/\hat{\sigma}_{i,j})^2}
\end{equation} 
By using the scaling factor, the concentration units on simulation results were arbitrary, which was consistent with the arbitrary units on the experimental training data. All simulation data was scaled by the corresponding $\beta_j$. 

We computed the Pareto rank of parameter set $\mathbf{k}_{i+1}$ by comparing the simulation error at iteration $i+1$ against the simulation archive, denoted as $\mathbf{K}_i$. We used the Fonseca and Fleming ranking scheme \cite{Fonseca1993} to estimate the rank of the parameter set $\mathbf{k}_{i+1}$. Parameter sets with increasing rank are progressively further away from the optimal trade-off surface. The parameter set $\mathbf{k}_{i+1}$ was accepted or rejected by the SA with probability $\mathcal{P}\left(\mathbf{k}_{i+1}\right)$: 
\begin{equation}\label{eqn_costMOSA}
\mathcal{P}(\mathbf{k}_{i+1}) \equiv \exp{\{-rank\left(\mathbf{k}_{i+1} \mid \mathbf{K}_{i} \right)/T\}}
\end{equation}
where $T$ is the computational annealing temperature. The Pareto rank for $\mathbf{k}_{i+1}$ is denoted by $rank\left(\mathbf{k}_{i+1}\mid \mathbf{K}_{i}\right)$. The annealing temperature was  adjusted according to the schedule
$T_k = \beta^k T_0$
where $\beta$ was defined as $\beta = \left(\frac{T_{f}}{T_{o}}\right)^{1/10}$. The initial temperature was given by $T_0 = n/log(2)$, with $n=4$ and the final temperature $T_f = 0.1$ used in this study.
The epoch-counter $k$ was incremented after the addition of 50 members to the ensemble. As the ensemble grew, the likelihood of accepting a high rank set decreased. Parameter sets were generated by applying a random perturbation in log space: 

\begin{equation}
\log\mathbf{k}_{i+1} = \log\mathbf{k}_i + \mathcal{N}(0,\nu)
\end{equation}
where $\mathcal{N}(0,\nu)$ is a normally distributed random number with zero mean and variance $\nu$, set as 0.1 in this model. The perturbation was applied in log space to account for large variation in parameter scales and to ensure positive parameter values. We used a local pattern search every $q$ steps, in our case 20, to minimize error for a single randomly selected objective. The local pattern-search algorithm used has been described previously \cite{Gadkar2003}.